name: Financial Crisis - Continuous Deployment

on:
  push:
    branches:
      - main
    paths:
      - "src/**"
      - "tests/**"
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.10"
  GCP_BUCKET: "mlops-financial-stress-data"

jobs:
  run-tests:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Run tests
        run: |
          if [ -d "tests" ]; then
            pytest tests/ || echo "No tests found or tests failed"
          else
            echo "No tests directory found - skipping"
          fi
        continue-on-error: true

  prepare-data:
    runs-on: ubuntu-latest
    needs: [run-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download data from GCS
        run: |
          echo "üì• Downloading data from GCS..."
          mkdir -p data/processed data/features data/splits
          
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/processed/features_engineered.csv data/processed/ || echo "File not found"
          gsutil -m cp gs://${{ env.GCP_BUCKET }}/data/splits/*.csv data/splits/ || echo "Splits not found, will generate"
          
          echo "‚úÖ Download complete"
      
      - name: Check what data we have
        run: |
          echo "Checking data files..."
          if [ -f "data/processed/features_engineered.csv" ]; then
            echo "‚úÖ features_engineered.csv found"
            ls -lh data/processed/features_engineered.csv
          fi
          
          if [ -f "data/splits/train_data.csv" ]; then
            echo "‚úÖ Splits already exist in GCS"
            ls -lh data/splits/
          else
            echo "‚ö†Ô∏è  Splits not found, need to generate"
          fi
      
      - name: Generate data splits (if needed)
        run: |
          if [ ! -f "data/splits/train_data.csv" ]; then
            echo "üìä Generating data splits..."
            
            python src/models/create_target.py
            python src/preprocessing/drop_features.py
            python src/preprocessing/temporal_split.py
            python src/preprocessing/handle_outliers_after_split.py
            
            echo "‚úÖ Splits generated"
            
            echo "üì§ Uploading splits to GCS..."
            gsutil -m cp data/splits/*.csv gs://${{ env.GCP_BUCKET }}/data/splits/
            gsutil cp data/features/quarterly_data_with_targets.csv gs://${{ env.GCP_BUCKET }}/data/features/
            echo "‚úÖ Uploaded to GCS"
          fi
        continue-on-error: true
      
      - name: Upload data splits as artifact
        uses: actions/upload-artifact@v4
        with:
          name: data-splits
          path: |
            data/splits/train_data.csv
            data/splits/val_data.csv
            data/splits/test_data.csv
          retention-days: 7
        continue-on-error: true

  train-model:
    runs-on: ubuntu-latest
    needs: [run-tests, prepare-data]
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download data splits
        uses: actions/download-artifact@v4
        with:
          name: data-splits
          path: data/splits
      
      - name: Verify data exists
        id: check_data
        run: |
          if [ -f "data/splits/train_data.csv" ]; then
            echo "data_exists=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Data splits found"
            ls -lh data/splits/
          else
            echo "data_exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è  Data splits not found"
          fi
      
      - name: Train XGBoost (revenue only)
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          python src/models/xgboost_model.py \
            --target revenue \
            --splits-dir data/splits \
            --output-dir models/xgboost \
            --no-mlflow
        continue-on-error: true
      
      - name: Check training results
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          if [ -f "models/xgboost/xgboost_revenue_metrics.json" ]; then
            echo "‚úÖ Model trained successfully!"
            echo ""
            echo "Model Metrics:"
            cat models/xgboost/xgboost_revenue_metrics.json
          else
            echo "‚ö†Ô∏è  Model training may have failed"
          fi
        continue-on-error: true
      
      - name: Upload model to GCS
        if: steps.check_data.outputs.data_exists == 'true'
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Upload model files to GCS
        if: steps.check_data.outputs.data_exists == 'true'
        run: |
          gcloud config set project ninth-iris-422916-f2
          gsutil -m cp models/xgboost/* gs://${{ env.GCP_BUCKET }}/models/xgboost/
          echo "‚úÖ Model uploaded to GCS"
        continue-on-error: true
      
      - name: Upload trained model as artifact
        if: steps.check_data.outputs.data_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: xgboost-revenue-model
          path: |
            models/xgboost/xgboost_revenue.pkl
            models/xgboost/xgboost_revenue_metrics.json
            models/xgboost/xgboost_revenue_importance.csv
          retention-days: 30
        continue-on-error: true

  # ==========================================================================
  # MODEL 3: ANOMALY DETECTION COMPLETE PIPELINE (NEW)
  # ==========================================================================
  train-model3-anomaly:
    runs-on: ubuntu-latest
    needs: [run-tests, prepare-data]
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download feature data
        run: |
          mkdir -p data/processed
          gsutil cp gs://${{ env.GCP_BUCKET }}/data/processed/features_engineered.csv data/processed/
          echo "‚úÖ Data downloaded"
      
      - name: Enable GCS for all configs
        run: |
          sed -i 's/source_type: "local"/source_type: "gcs"/' configs/eda_config.yaml
          sed -i 's/source_type: "local"/source_type: "gcs"/' configs/model_config.yaml
          sed -i 's/upload_to_gcs: false/upload_to_gcs: true/' configs/eda_config.yaml
          sed -i 's/upload_to_gcs: false/upload_to_gcs: true/' configs/model_config.yaml
      
      - name: Run EDA
        run: |
          echo "üìä Step 1: EDA with threshold extraction..."
          mkdir -p outputs/eda/{plots,data,reports}
          python src/eda/eda.py
          echo "‚úÖ EDA complete"
        continue-on-error: true
      
      - name: Extract thresholds
        run: |
          echo "üî¢ Step 2: Threshold extraction..."
          python src/labeling/auto_threshold_extractor.py
          [ -f "outputs/snorkel/data/extracted_thresholds.json" ] && cat outputs/snorkel/data/extracted_thresholds.json
        continue-on-error: true
      
      - name: Run Snorkel labeling
        run: |
          echo "üè∑Ô∏è  Step 3: Snorkel labeling..."
          mkdir -p data/labeled outputs/snorkel/{data,plots,reports}
          python src/labeling/snorkel_pipeline.py
          
          if [ -f "outputs/snorkel/data/snorkel_labeled_only.csv" ]; then
            python -c "import pandas as pd; df=pd.read_csv('outputs/snorkel/data/snorkel_labeled_only.csv'); print(f'Labeled: {len(df):,}, At-risk: {df[\"AT_RISK\"].sum():,} ({df[\"AT_RISK\"].sum()/len(df):.2%})')"
          fi
        continue-on-error: true
      
      - name: Train anomaly models
        run: |
          echo "ü§ñ Step 4: Training anomaly detection models..."
          mkdir -p models/{anomaly_detection,Ensemble} outputs/models/{plots,reports,results}
          python src/models/train_anomaly_detection.py
        continue-on-error: true
      
      - name: Display Model 3 results
        run: |
          echo "================================================================"
          echo "MODEL 3 RESULTS"
          echo "================================================================"
          [ -f "models/Ensemble/ensemble_weights.json" ] && cat models/Ensemble/ensemble_weights.json
          [ -f "outputs/models/results/ensemble_comparison.csv" ] && cat outputs/models/results/ensemble_comparison.csv
        continue-on-error: true
      
      - name: Upload Model 3 artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model3-complete
          path: |
            models/anomaly_detection/
            models/Ensemble/
            outputs/models/
            outputs/snorkel/data/snorkel_labeled_only.csv
          retention-days: 30
        continue-on-error: true

  pipeline-summary:
    runs-on: ubuntu-latest
    needs: [run-tests, prepare-data, train-model, train-model3-anomaly]
    if: always()
    
    steps:
      - name: Display summary
        run: |
          echo "================================================================"
          echo "COMPLETE CI/CD PIPELINE SUMMARY"
          echo "================================================================"
          echo ""
          echo "Pipeline Results:"
          echo "  Tests:              ${{ needs.run-tests.result }}"
          echo "  Data Preparation:   ${{ needs.prepare-data.result }}"
          echo "  Model 2 (XGBoost):  ${{ needs.train-model.result }}"
          echo "  Model 3 (Anomaly):  ${{ needs.train-model3-anomaly.result }}"
          echo ""
          if [ "${{ needs.train-model.result }}" == "success" ] && [ "${{ needs.train-model3-anomaly.result }}" == "success" ]; then
            echo "‚úÖ ALL PIPELINES COMPLETED SUCCESSFULLY!"
            echo ""
            echo "GCS Storage: gs://mlops-financial-stress-data/"
            echo "  ‚îú‚îÄ‚îÄ models/xgboost/         (Model 2)"
            echo "  ‚îú‚îÄ‚îÄ models/model3/          (Model 3 - Anomaly Detection)"
            echo "  ‚îú‚îÄ‚îÄ outputs/snorkel/        (Labeled data)"
            echo "  ‚îî‚îÄ‚îÄ outputs/model3/         (Reports & analysis)"
          